


# Import libraries
from textblob import TextBlob 
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
import matplotlib
import nltk
import re
nltk.download('stopwords')
nltk.download('punkt')
nltk.download('averaged_perceptron_tagger')
from collections import Counter
sns.set()





# Loading 20th Century text data
myfile = open('20th_Century_Wiki_refined.txt', encoding='utf-8')


# Import txt file

with open('20th_Century_Wiki_refined.txt', 'r', errors='ignore') as file:
    data = file.read().replace('\n', '')


# Sentence tokenization 
from nltk.tokenize import sent_tokenize
tokenized_sent = sent_tokenize(data)
print(tokenized_sent)


# Word tokenization
from nltk.tokenize import word_tokenize
tokenized_word = word_tokenize(data)
print(tokenized_word)


# Create frequency distribution
from nltk.probability import FreqDist
dist_words = FreqDist(tokenized_word)
print(dist_words)


# Check top 10 most common words
top10 = dist_words.most_common(10)
print(top10)


# Create a dataframe from top 10 list
df_top10 = pd.DataFrame(top10, columns=['Word', 'Frequency'])
print(df_top10)


# Create a bar chart of 10 most common words
plt.figure(figsize=(10, 6))
df_top10.plot(kind='bar', x='Word', y='Frequency', legend=False, color='blue')
plt.title('Top 10 Most Common Words')
plt.xlabel('Words')
plt.ylabel('Frequency')
plt.show()





# Defining stopwords
from nltk.corpus import stopwords
stop_words = set(stopwords.words("english"))
print(stop_words)


# Removing stopwords in words
filtered_words = [] # creates an empty list
for word in tokenized_word:
    if word not in stop_words:
        filtered_words.append(word)


# Check filtered_words
filtered_words


# Substitute all punctuation marks with a space 
sans_punc = re.sub("[^a-zA-Z0-9]",  # Search for all non-letters and non-digits
                          " ",          # Replace all non-letters and non-digits with spaces
                          str(filtered_words))





# Check list without stop words and punctuation
sans_punc


# Re-tokenize our new list by words
tokenized_word_2 = word_tokenize(sans_punc)
print(tokenized_word_2)


# Create a new FreqDist
dist_words_filter = FreqDist(tokenized_word_2)
print(dist_words_filter)


# Check top 10 most common words in new list
top10_filter = dist_words_filter.most_common(10)
print(top10_filter)








# Loading cleaned 20th Century text data
myfile_2 = open('20th_Century_Wiki_refined.txt', encoding='utf-8')


# Import txt file
with open('20th_Century_Wiki_refined.txt', 'r', errors='ignore') as file:
    data_2 = file.read().replace('\n', '')


# Sentence tokenization 
from nltk.tokenize import sent_tokenize
tokenized_sent_new = sent_tokenize(data_2)
print(tokenized_sent_new)


# Word tokenization
from nltk.tokenize import word_tokenize
tokenized_word_new = word_tokenize(data_2)
print(tokenized_word_new)


# Create frequency distribution
from nltk.probability import FreqDist
dist_words_new = FreqDist(tokenized_word_new)
print(dist_words_new)


# Check top 10 most common words
top10_new = dist_words.most_common(10)
print(top10_new)


# Create a dataframe from new top 10 list
df_top10_new = pd.DataFrame(top10_new, columns=['Word', 'Frequency'])
print(df_top10_new)


# Create a bar chart of 10 most common words
plt.figure(figsize=(10, 6))
df_top10_new.plot(kind='bar', x='Word', y='Frequency', legend=False, color='blue')
plt.title('Top 10 Most Common Words')
plt.xlabel('Words')
plt.ylabel('Frequency')
plt.show()








# Removing stopwords in words
filtered_words_new = [] # creates an empty list
for word in tokenized_word_new:
    if word not in stop_words:
        filtered_words_new.append(word)


# Check filtered_words_new
filtered_words_new


# Substitute all punctuation marks with a space 
sans_punc_new = re.sub("[^a-zA-Z0-9]",  # Search for all non-letters and non-digits
                          " ",          # Replace all non-letters and non-digits with spaces
                          str(filtered_words_new))





# Check list without stop words and punctuation
sans_punc_new


# Re-tokenize our new list by words
tokenized_word_new_2 = word_tokenize(sans_punc_new)
print(tokenized_word_new_2)


# Create a new FreqDist
dist_words_filter_new = FreqDist(tokenized_word_new_2)
print(dist_words_filter_new)


# Check top 10 most common words in new list
top10_filter_new = dist_words_filter_new.most_common(10)
print(top10_filter_new)


# Removing additional stop words
new_stopwords = ['The', 's', 'In', 'I', 'It', 'And', 'edit', 'would']

filtered_words_new_2 = []
for word in tokenized_word_new_2:
    if word not in new_stopwords:
        filtered_words_new_2.append(word)
print(filtered_words_new_2)


# Create a new FreqDist
dist_words_filter_new_2 = FreqDist(filtered_words_new_2)
print(dist_words_filter_new_2)


# Check top 10 most common words in new filtered list
top10_filter_new_2 = dist_words_filter_new_2.most_common(10)
print(top10_filter_new_2)


# Create a dataframe from new top 10 list
df_top10_new_2 = pd.DataFrame(top10_filter_new_2, columns=['Word', 'Frequency'])
print(df_top10_new_2)


# Create a bar chart of 10 most common words
plt.figure(figsize=(10, 6))
df_top10_new_2.plot(kind='bar', x='Word', y='Frequency', legend=False, color='blue')
plt.title('Top 10 Most Common Words')
plt.xlabel('Words')
plt.ylabel('Frequency')
plt.show()





# Defining special cases list to preserve their original case
special_cases = ['US']
# Convert words to lowercase based on conditions
processed_words = [word if word in special_cases else word.lower() for word in filtered_words_new_2]


# Create a FreqDist of processed words
dist_words_processed = FreqDist(processed_words)
print(dist_words_processed)


# Check top 10 most common words in processed words list
top10_processed = dist_words_processed.most_common(10)
print(top10_processed)


# Create a dataframe from processed top 10 list
df_top10_processed = pd.DataFrame(top10_processed, columns=['Word', 'Frequency'])
print(df_top10_processed)


# Create a bar chart of 10 most common words in processed list
plt.figure(figsize=(10, 6))
df_top10_processed.plot(kind='bar', x='Word', y='Frequency', legend=False, color='blue')
plt.title('Top 10 Most Common Words')
plt.xlabel('Words')
plt.ylabel('Frequency')
plt.show()








# Check time it will take to create TextBlob text
%time
text = TextBlob(str(filtered_words_new_2))


# Checking TextBlob text
text


# Adding tags to the list
tags_list = text.tags


# Reviewing tags_list
tags_list


# Create a dataframe of the words and their types
df_text = pd.DataFrame(tags_list)
df_text.columns = ['Words', "Word type"]


# Checking dataframe
df_text.head()


# Grouping dataframe by word type to get counts
df_t = df_text.groupby('Word type').count().reset_index()


# Checking grouped dataframe
df_t.head()


# Finding top 10 POS tags
pos_top10 = df_t.nlargest(10, 'Words')


# Plotting bar chart for top 10 POS tags
plt.figure(figsize = (10, 5))
with sns.dark_palette("xkcd:blue", 20):
    sns.barplot(x = "Words", y = "Word type",
    saturation = 0.9, data = pos_top10).set_title("20th Century Events - top 10 word types used")








# Creating df of nouns and their occurrences then finding top 15
df_nouns = df_text[(df_text['Word type'] == "NN") | (df_text['Word type'] == "NNS") | (df_text['Word type'] == "NNP") | (df_text['Word type'] == "NNPS")]
df_nouns.columns = ["Word", "Occurrences"]
x = df_nouns.groupby('Word').count().reset_index()
y = x.sort_values(by = ['Occurrences'], ascending=False)
top15_nouns = y.nlargest(15, 'Occurrences')


# Checking top15_nouns list
top15_nouns


# Plotting a bar chart of the most frequently used nouns
plt.figure(figsize=(15, 6))
with sns.dark_palette("xkcd:blue", 15):
    sns.barplot(x="Word", y="Occurrences",
    saturation=0.9, data = top15_nouns).set_title("20th Century Events - most frequently used nouns")








# Creating df of verbs and their occurrences then finding top 15
df_verbs = df_text[(df_text['Word type'] == "VB")  | (df_text['Word type'] == "VBD") | (df_text['Word type'] == "VBG") | (df_text['Word type'] == "VBN")]
df_verbs.columns = ["Word", "Occurrences"]
x = df_verbs.groupby('Word').count().reset_index()
y = x.sort_values(by = ['Occurrences'], ascending=False)
top15_verbs = y.nlargest(15, 'Occurrences')


# Checking top15_verbs list
top15_verbs


# Plotting a bar chart of the most frequently used verbs
plt.figure(figsize=(15, 6))
with sns.dark_palette("xkcd:red", 15):
    sns.barplot(x="Word", y="Occurrences",
    saturation=0.9, data = top15_verbs).set_title("20th Century Events - most frequently used verbs")








# Creating df of adjectives and their occurrences then finding top 15
df_adj = df_text[(df_text['Word type'] == "JJ") | (df_text['Word type'] == "JJR") | (df_text['Word type'] == "JJS")]
df_adj.columns = ["Word", "Occurrences"]
x = df_adj.groupby('Word').count().reset_index()
y = x.sort_values(by=['Occurrences'], ascending=False)
top15_adj = y.nlargest(15, 'Occurrences')


# Checking top15_adj list
top15_adj


# Plotting a bar chart of the most frequently used adjectives
plt.figure(figsize=(15, 6))
with sns.dark_palette("xkcd:green", 15):
    sns.barplot(x="Word", y="Occurrences",
    saturation=0.9, data = top15_adj).set_title("20th Century Events - most frequently used adjectives")









